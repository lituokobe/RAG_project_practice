{
    "id": null,
    "metadata": {
        "source": "../datas/layout-parser-paper.pdf",
        "detection_class_prob": 0.9529311060905457,
        "coordinates": {
            "points": [
                [
                    374.3472222222222,
                    1302.9837344444443
                ],
                [
                    374.3472222222222,
                    1529.910401111111
                ],
                [
                    1339.5860188733889,
                    1529.910401111111
                ],
                [
                    1339.5860188733889,
                    1302.9837344444443
                ]
            ],
            "system": "PixelSpace",
            "layout_width": 1700,
            "layout_height": 2200
        },
        "links": [
            {
                "text": "32",
                "url": "cite.shen2020olala",
                "start_index": 116
            }
        ],
        "last_modified": "2025-04-07T09:54:10",
        "filetype": "application/pdf",
        "languages": [
            "eng"
        ],
        "page_number": 9,
        "file_directory": "../datas",
        "filename": "layout-parser-paper.pdf",
        "category": "NarrativeText",
        "element_id": "a3498730b5cd3fe9405fad69bcf37882"
    },
    "page_content": "LayoutParser incorporates a toolkit optimized for annotating document lay- outs using object-level active learning [32]. With the help from a layout detection model trained along with labeling, only the most important layout objects within each image, rather than the whole image, are required for labeling. The rest of the regions are automatically annotated with high conﬁdence predictions from the layout detection model. This allows a layout dataset to be created more eﬃciently with only around 60% of the labeling budget.",
    "type": "Document"
}